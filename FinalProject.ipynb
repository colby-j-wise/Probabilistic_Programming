{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Trips and Traffic\n",
    "\n",
    "Where most models use real-time data from users to predict arrival\n",
    "times at any given moment, we believe they could be improved by including a predictive\n",
    "element. Our intent is to use the NYC Taxi and Limousine Commission's yellow and green cab data set to estimate density of pickup and dropoffs at any given place and time. We will then use the density as a proxy for traffic to estimate the time it takes to arrive at a destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import edward as ed\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.setup import set_random_seeds\n",
    "import data\n",
    "# set_random_seeds(42)\n",
    "plt.style.use(\"seaborn-talk\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Use the\n",
    "[2015 NYC Yellow Cab Dataset](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml),\n",
    "which consists of pickup and dropoff coordinates for trips, along \n",
    "with metadata like cost, distance, and number of passengers. For the time being, we are only interested in trips that occurr entirely within the range from 1st street to 59th street, and 12th ave to 1st ave, during the month of april. This data can be easily visualized as 2d plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```SQL\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  [bigquery-public-data:new_york.tlc_yellow_trips_2016]\n",
    "WHERE\n",
    "  (pickup_longitude > -74.0124053955\n",
    "    AND pickup_longitude < -73.9673309326)\n",
    "  AND (pickup_latitude > 40.7186431885\n",
    "    AND pickup_latitude < 40.7735137939)\n",
    "  AND (dropoff_longitude > -74.0124053955\n",
    "    AND dropoff_longitude < -73.9673309326)\n",
    "  AND (dropoff_latitude > 40.7186431885\n",
    "    AND dropoff_latitude < 40.7735137939)\n",
    "  AND TIMESTAMP_TO_MSEC(pickup_datetime) > 1459483200000\n",
    "  AND TIMESTAMP_TO_MSEC(dropoff_datetime) < 1462075200000\n",
    "  order by pickup_datetime desc\n",
    "LIMIT\n",
    "  10000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.get_data('datasets/results-20171101-101319.csv', name=\"raw_coordinates\")\n",
    "with tf.Session() as sess:\n",
    "    rotated_data = sess.run(data.rotate(dataset, np.pi * 0.0))\n",
    "sns.jointplot(x=rotated_data[0, :], y=rotated_data[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = data.bin_2d(rotated_data.T, 25)\n",
    "sns.heatmap(H.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this visualization, it looks as though the density of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a placeholder `X`. During inference, we pass in\n",
    "the value for this placeholder according to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Perform variational inference.\n",
    "Define the variational model to be a fully factorized normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qf = Normal(loc=tf.Variable(tf.random_normal([N])),\n",
    "            scale=tf.nn.softplus(tf.Variable(tf.random_normal([N]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run variational inference for `500` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 82.755\n"
     ]
    }
   ],
   "source": [
    "inference = ed.KLqp({f: qf}, data={X: X_train, y: y_train})\n",
    "inference.run(n_iter=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case\n",
    "`KLqp` defaults to minimizing the\n",
    "$\\text{KL}(q\\|p)$ divergence measure using the reparameterization\n",
    "gradient.\n",
    "For more details on inference, see the [$\\text{KL}(q\\|p)$ tutorial](/tutorials/klqp).\n",
    "(This example happens to be slow because evaluating and inverting full\n",
    "covariances in Gaussian processes happens to be slow.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
