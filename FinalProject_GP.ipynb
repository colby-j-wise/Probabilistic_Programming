{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import edward as ed\n",
    "from edward.models import Normal\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# our functions\n",
    "import setup\n",
    "import data\n",
    "import visualizations\n",
    "import basis_functions\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "setup.set_random_seeds(42)\n",
    "plt.style.use(\"seaborn-talk\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manhattan = data.get_borough_data(\"data/preprocessed.csv\", \"Manhattan\")\n",
    "ues_to_msh = data.get_neighborhood_to_neighborhood(\"Morningside Heights\", \"Upper East Side-Carnegie Hill\", manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicator_cols = [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\", \"manhattan_distance\", \"pickup_hour\", \"pickup_timestamp\"]\n",
    "y_cols = [\"trip_duration\"]\n",
    "\n",
    "x_train_raw, y_train_raw, x_test_raw, y_test_raw = data.train_test_split(ues_to_msh, 0.1, indicator_cols, y_cols)\n",
    "x_train = data.standardize_cols(x_train_raw)\n",
    "x_test = data.standardize_cols(x_test_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box's Loop - Iteration 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Gaussian Process\n",
    "\n",
    "Gaussian processes (GPs) are a supervised machine learning algorithm that measures the similarity between input data points, using a predefined kernel function, to predict the value of an unseen data point. We like to think about GPs as: defining an infinite distribution on functions over a continuous sapce where we observe some data points and want to assign probabilities to all the ways a line could be drawn through those points. The idea is that we hope these lines (or functions) we draw are similar with high probability to that true function which we will never know. \n",
    "\n",
    "Drawing a parallel to our polynomial basis GLM example: instead of choosing a polynomial basis function of degree four, with GPs using a kernel function we'd like to infer the true function from our data. Hence, GPs allow us retain the flexibility of capturing non-linearities in our data but accounting for \"infinite\" numbers of basis functions. \n",
    "\n",
    "### Interpretation of Kernel Function\n",
    "\n",
    "GPs are parameterized by a pre-determined *kernel function* which is positive-semidefinite covariance matrix that calculated distanced between every pair of $N$ observed points. The Kernel function must be a square matrix and allows use to explore *smoothness* and *periodicity* in our observed data. \n",
    "\n",
    "In our analysis we will explore a few kernel functions but note there are many popular kernel functions, some more applicable than others and exploring them all was impossible. \n",
    "\n",
    "### Model Overview\n",
    "\n",
    "A GP can be specified entirely by it's Kernel Function and mean (often assumed to 0): \n",
    "\n",
    "Where,\n",
    "\n",
    "**Prior:** $p(f) = GP(\\ 0,\\ K(x,x',\\theta)\\ )$ where $\\theta$ = *length_scale* $l_f$, sigma $\\sigma_f$\n",
    "\n",
    "**Likelihood:**  $p(y\\ |\\ f,\\ X,\\ \\theta) = GP(\\ f,\\ K(x,x', \\theta)\\ )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.79125001,  18.39803253])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gp_reg_invert_K(x, y, x_star, kernel):\n",
    "    K = kernel(x)\n",
    "    k_star = kernel(x_star, x)\n",
    "    k_star_star = kernel(x_star, x_star)\n",
    "    mu_n, sigma_n_sqr = tf.nn.moments(y, 1)\n",
    "    K_noisy = K + sigma_n_sqr * tf.eye(tf.cast(x.get_shape()[0], tf.int32), dtype=tf.float64)\n",
    "    K_noisy_inv = tf.matrix_inverse(K_noisy)\n",
    "    f_bar = tf.matmul(tf.matmul(k_star, K_noisy_inv), y)\n",
    "    tmp = tf.matmul(tf.matmul(k_star, K_noisy_inv), k_star, transpose_b=True)\n",
    "    v = k_star_star - tmp\n",
    "    return f_bar, v\n",
    "\n",
    "y = tf.constant(y_train_raw.as_matrix())\n",
    "x = tf.constant(x_train)\n",
    "x_star = tf.constant(x_test.iloc[:2, :])\n",
    "\n",
    "mu, var = tf.nn.moments(y, 1)\n",
    "\n",
    "def my_cov(X, X2=None, lengthscale=1.0, variance=1.0):\n",
    "    lengthscale = tf.convert_to_tensor(lengthscale, dtype=tf.float64)\n",
    "    variance = tf.convert_to_tensor(variance, dtype=tf.float64)\n",
    "\n",
    "    X = tf.convert_to_tensor(X)\n",
    "    X = X / lengthscale\n",
    "    Xs = tf.reduce_sum(tf.square(X), 1)\n",
    "    if X2 is None:\n",
    "        X2 = X\n",
    "        X2s = Xs\n",
    "    else:\n",
    "        X2 = tf.convert_to_tensor(X2)\n",
    "        X2 = X2 / lengthscale\n",
    "        X2s = tf.reduce_sum(tf.square(X2), 1)\n",
    "\n",
    "    square = tf.reshape(Xs, [-1, 1]) + tf.reshape(X2s, [1, -1]) - 2 * tf.matmul(X, X2, transpose_b=True)\n",
    "    output = variance * tf.exp(-square / 2)\n",
    "    return output\n",
    "\n",
    "f, v = gp_reg_invert_K(x, y, x_star, my_cov)\n",
    "mean = np.reshape(f.eval(session=tf.Session()), -1)\n",
    "covariance = v.eval(session=tf.Session())\n",
    "np.random.multivariate_normal(mean, covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "# def gp_reg(X, y, kernel, sigma_sqr_n, x_star):\n",
    "#     K = kernel(X)\n",
    "#     noise = sigma_sqr_n * tf.eye(tf.cast(X.get_shape()[0], tf.int32), dtype=tf.float64)\n",
    "#     K_noisy = K + noise\n",
    "#     L = tf.cholesky(K_noisy)\n",
    "#     z = tf.cholesky_solve(L, y)\n",
    "#     alpha = tf.cholesky_solve(tf.matrix_transpose(L), z)\n",
    "#     xs = tf.unstack(x_star)\n",
    "#     fs, vs = [], []\n",
    "#     k_star = kernel(X, x)\n",
    "#     f = tf.matmul(k_star, alpha, transpose_a=True)\n",
    "#     print(f.eval(session=tf.Session()))\n",
    "#     f = \n",
    "#     for idx, x in enumerate(xs):\n",
    "#         k_star = kernel(x, X)\n",
    "#         print(k_star)\n",
    "#         return\n",
    "#         k_star_star = kernel(x_star, x_star)\n",
    "#         fs.append(tf.matmul(k_star, alpha))\n",
    "#         v = tf.cholesky_solve(L, tf.reshape(k_star[idx], [633, 1]))\n",
    "#         print(v)\n",
    "#         print(tf.matmul(v, v, transpose_a=True))\n",
    "#         print(k_star_star)\n",
    "#         vs.append(k_star_star - tf.mat)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
