{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Trips and Traffic\n",
    "\n",
    "Where most models use real-time data from users to predict arrival\n",
    "times at any given moment, we believe they could be improved by including a predictive\n",
    "element. Our intent is to use the NYC Taxi and Limousine Commission's yellow and green cab data set to estimate density of pickup and dropoffs at any given place and time. We will then use the density as a proxy for traffic to estimate the time it takes to arrive at a destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import edward as ed\n",
    "from edward.models import Normal\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# our functions\n",
    "from setup import set_random_seeds\n",
    "from data import get_borough_data\n",
    "from visualizations import visualize_by_borough\n",
    "set_random_seeds(42)\n",
    "plt.style.use(\"seaborn-talk\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We used the\n",
    "[2015 NYC Yellow Cab Dataset](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml),\n",
    "which consists of pickup and dropoff coordinates for trips, along \n",
    "with metadata like cost, distance, and number of passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan = get_borough_data(\"data/preprocessed.csv\", \"Manhattan\")\n",
    "indices = np.random.choice(manhattan.shape[0], size=10000, replace=False)\n",
    "axes = visualize_by_borough(manhattan.iloc[indices, :])\n",
    "plt.show(axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will begin by trying to model the trip duration between two neighborhoods. Specifically, Morningside Heights and Upper East Side-Carnegie Hill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighborhood_to_neighborhood(source_neighborhood, sink_neighborhood, full_dataset):\n",
    "    x = full_dataset.where((full_dataset[\"pickup_neighborhood_name\"] == source_neighborhood) &\n",
    "                           (full_dataset[\"dropoff_neighborhood_name\"] == sink_neighborhood)).dropna()\n",
    "    return x\n",
    "    \n",
    "def add_arrival_timestamp(x):\n",
    "    x[\"dropoff_timestamp\"] = x[\"pickup_timestamp\"] + x[\"trip_duration\"]\n",
    "    return x\n",
    "\n",
    "def add_dropoff_datetime(x):\n",
    "    # This takes a while...\n",
    "    x[\"dropoff_datetime\"] = x[\"pickup_datetime\"] + x[\"trip_duration\"].apply(lambda x: pd.Timedelta(seconds=x))\n",
    "    return x\n",
    "\n",
    "def add_dropoff_hour(x):\n",
    "    if \"dropoff_datetime\" not in x.columns:\n",
    "        x = add_dropoff_datetime(x)\n",
    "    x[\"dropoff_hour\"] = x[\"dropoff_datetime\"].apply(lambda x: x.hour)\n",
    "    return x\n",
    "    \n",
    "def add_pickup_hour(x):\n",
    "    x[\"pickup_hour\"] = x[\"pickup_datetime\"].apply(lambda x: x.hour)\n",
    "    return x\n",
    "\n",
    "# manhattan = add_arrival_timestamp(manhattan)\n",
    "# ues_to_msh = get_neighborhood_to_neighborhood(\"Morningside Heights\", \"Upper East Side-Carnegie Hill\", manhattan)\n",
    "# ues_to_msh = add_dropoff_hour(ues_to_msh)\n",
    "# ues_to_msh = add_pickup_hour(ues_to_msh)\n",
    "# visualize_by_borough(ues_to_msh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets graph the trip duration by hour of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ues_to_msh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8e15fc56a590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# note that the selection of the trip_duration column here is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# arbitrary, the count would be the same regardless of column selected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mavg_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mues_to_msh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropoff_hour\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trip_duration\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avg_trip_duration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mavg_duration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hour\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hour\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"avg_trip_duration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavg_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ues_to_msh' is not defined"
     ]
    }
   ],
   "source": [
    "# note that the selection of the trip_duration column here is \n",
    "# arbitrary, the count would be the same regardless of column selected\n",
    "avg_duration = ues_to_msh.groupby(\"dropoff_hour\")[\"trip_duration\"].mean().to_frame(\"avg_trip_duration\")\n",
    "avg_duration[\"hour\"] = range(24)\n",
    "sns.lmplot(x=\"hour\", y=\"avg_trip_duration\", data=avg_duration, fit_reg=False)\n",
    "plt.show()\n",
    "sns.lmplot(x=\"dropoff_hour\", y=\"trip_duration\", data=ues_to_msh, fit_reg=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to remove some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(data, col, frac_stddev):\n",
    "    bound = frac_stddev * data.describe()[col][\"std\"]\n",
    "    return data.where(np.abs(data[col]) < bound).dropna()\n",
    "# ues_to_msh = remove_outliers(ues_to_msh, \"trip_duration\", 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"dropoff_hour\", y=\"trip_duration\", data=ues_to_msh, fit_reg=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, there are also a bunch of -1's. Maybe missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_leq_zero(data, col):\n",
    "    return data.where(data[col] > 0).dropna()\n",
    "# ues_to_msh = remove_leq_zero(ues_to_msh, \"trip_duration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"dropoff_hour\", y=\"trip_duration\", data=ues_to_msh, fit_reg=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add manhattan distance to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_manhattan_distance(data):\n",
    "    data[\"manhattan_distance\"] = abs(data[\"pickup_longitude\"] - data[\"dropoff_longitude\"] + \\\n",
    "                                 data[\"pickup_latitude\"] - data[\"dropoff_latitude\"])\n",
    "    return data\n",
    "\n",
    "from haversine import haversine\n",
    "\n",
    "def add_manhattan_distance2(data):\n",
    "    \n",
    "    def haversineDistance(row):\n",
    "        pickup_point = (row['pickup_latitude'], row['pickup_longitude'])\n",
    "        dropoff_point = (row['dropoff_latitude'], row['dropoff_longitude'])\n",
    "        # Return distance in miles rounded to 2 decimals\n",
    "        return round( haversine(pickup_point, dropoff_point, miles=True), 2)\n",
    "    \n",
    "    data['manhattan_distance'] = data.apply(lambda row: haversineDistance(row),axis=1)\n",
    "    return data[(0 < data.manhattan_distance) & (data.manhattan_distance < 50)]\n",
    "\n",
    "def add_trip_speed(data):\n",
    "    data['trip_speed_mph'] = data['manhattan_distance'] / data['trip_duration']\n",
    "    return data[(0 < data.trip_speed_mph) & (data.trip_speed_mph < 150)]\n",
    "\n",
    "# def add_day_of_week(data, includeNames=False):\n",
    "#     dayDict = ({0:'Monday',1:'Tuesday',2:'Wednesday',\n",
    "#                 3:'Thursday', 4:'Friday',5:'Saturday',6:'Sunday'})\n",
    "        \n",
    "#     if includeNames:\n",
    "#         data['day_idx'] = pd.to_datetime(df.pickup_date, format='%Y-%m-%d').dt.dayofweek\n",
    "#         data['weekday'] = data.loc[:, ('day_idx')].apply(lambda row: dayDict.get(row))\n",
    "#     else:\n",
    "#         data['day'] = pd.to_datetime(data.pickup_date, format='%Y-%m-%d').dt.dayofweek\n",
    "#     return data\n",
    "\n",
    "# def add_all_time(data):\n",
    "#     data['pickup_datetime'] = pd.to_datetime(data.loc['pickup_datetime'])\n",
    "#     data['pickup_date'] = data.loc['pickup_datetime'].dt.date\n",
    "#     data['pickup_time'] = data.loc['pickup_datetime'].dt.time\n",
    "#     return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manhattan = get_borough_data(\"data/preprocessed.csv\", \"Manhattan\")\n",
    "manhattan = manhattan[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude\n",
       "0        -73.982155        40.767937         -73.964630         40.765602\n",
       "1        -73.980415        40.738564         -73.999481         40.731152\n",
       "2        -73.979027        40.763939         -74.005333         40.710087\n",
       "3        -74.010040        40.719971         -74.012268         40.706718\n",
       "4        -73.973053        40.793209         -73.972923         40.782520"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#manhattan = add_arrival_timestamp(manhattan)\n",
    "#manhattan = add_pickup_hour(manhattan)\n",
    "#manhattan = remove_outliers(manhattan, \"trip_duration\", 1.0)\n",
    "#manhattan = remove_leq_zero(manhattan, \"trip_duration\")\n",
    "#manhattan = add_manhattan_distance2(manhattan)\n",
    "#manhattan = add_day_of_week(manhattan)\n",
    "\n",
    "x_ = manhattan.loc[:, [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]]\n",
    "\n",
    "# x_[\"pickup_longitude\"] = x_.pickup_longitude.round(3)\n",
    "# x_[\"pickup_latitude\"] = x_.pickup_longitude.round(3)\n",
    "# x_[\"dropoff_longitude\"] = x_.pickup_longitude.round(3)\n",
    "# x_[\"dropoff_latitude\"] = x_.pickup_longitude.round(3)\n",
    "x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime\n",
       "0 2016-03-14 17:24:55\n",
       "1 2016-06-12 00:43:35\n",
       "2 2016-01-19 11:35:24\n",
       "3 2016-04-06 19:32:31\n",
       "4 2016-03-26 13:30:55"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ = manhattan.loc[:, [\"pickup_datetime\"]]\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pairwise \n",
      " [[ 0.          0.00326725  0.00476443  0.00881499  0.00107654]\n",
      " [ 0.00326725  0.          0.0011238   0.00198381  0.00638441]\n",
      " [ 0.00476443  0.0011238   0.          0.00295448  0.00718942]\n",
      " [ 0.00881499  0.00198381  0.00295448  0.          0.01402585]\n",
      " [ 0.00107654  0.00638441  0.00718942  0.01402585  0.        ]]\n",
      "\n",
      " exp pairwise \n",
      " [[ 1.          0.99673809  0.9952469   0.99122374  0.99892404]\n",
      " [ 0.99673809  1.          0.99887683  0.99801815  0.99363593]\n",
      " [ 0.9952469   0.99887683  1.          0.99704988  0.99283636]\n",
      " [ 0.99122374  0.99801815  0.99704988  1.          0.98607205]\n",
      " [ 0.99892404  0.99363593  0.99283636  0.98607205  1.        ]]\n",
      "\n",
      " cholesky \n",
      " [[ 1.          0.          0.          0.          0.        ]\n",
      " [ 0.99673809  0.08070432  0.          0.          0.        ]\n",
      " [ 0.9952469   0.0852041   0.04715797  0.          0.        ]\n",
      " [ 0.99122374  0.12425229 -0.00104501  0.04511947  0.        ]\n",
      " [ 0.99892404 -0.02514992  0.01703172 -0.02088771  0.0281401 ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "# this is an NxD matrix, where N is number of items and D its dimensionalites\n",
    "sigma = 1.0\n",
    "n = x_.shape[0]\n",
    "pairwise_sq_dists = squareform(pdist(x_, 'sqeuclidean'))\n",
    "print(\"\\n pairwise \\n\", pairwise_sq_dists)\n",
    "\n",
    "print(\"\\n exp pairwise \\n\", np.exp(-pairwise_sq_dists))\n",
    "K = np.exp(-pairwise_sq_dists / sigma**2)\n",
    "\n",
    "L = np.linalg.cholesky(K + 1e-15*np.eye(n))\n",
    "print(\"\\n cholesky \\n\", L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not correct -- see pairwise below\n",
    "def rbf_kernel(X, gamma=-1.0):\n",
    "    # Gaussian (RBF) kernel k(X,X')\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    gamma = tf.constant(gamma)\n",
    "    sq_vec = tf.multiply(2., tf.matmul(X, tf.transpose(X)))\n",
    "    kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_vec)))\n",
    "    return kernel\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def pairwise_sq_dist_kernel(X, sigma = 1.0):\n",
    "    pairwise_sq_dists = squareform(pdist(x_, 'sqeuclidean'))\n",
    "    K = np.exp(-pairwise_sq_dists / sigma**2)\n",
    "    return tf.convert_to_tensor(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X input:\n",
      " [[-73.98  40.77 -73.96  40.77]\n",
      " [-73.98  40.74 -74.    40.73]\n",
      " [-73.98  40.76 -74.01  40.71]\n",
      " [-74.01  40.72 -74.01  40.71]\n",
      " [-73.97  40.79 -73.97  40.78]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.78826945,  1.72643273,  1.76500818,  1.62409339,  1.82587397])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not needed but literally checked to make sure the data subset was PSD\n",
    "# since tf.cholesky requires square + PSD matrix\n",
    "def is_positive_definite(x):\n",
    "    print(\"Is postive semi def: \",np.all(np.linalg.eigvals(x) > 0))\n",
    "\n",
    "a = x_.round(2)\n",
    "print(\"X input:\\n\", a)\n",
    "\n",
    "b = tf.convert_to_tensor( x_.round(2) )\n",
    "Y = pairwise_sq_dist_kernel(b)\n",
    "Y.eval()\n",
    "\n",
    "#chol = tf.cholesky(tf.convert_to_tensor(Y))\n",
    "#chol.eval()\n",
    "\n",
    "f = MultivariateNormalTriL(loc=tf.cast(tf.zeros(5), tf.float64), scale_tril=tf.cholesky(tf.convert_to_tensor(Y)))\n",
    "f.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from observations import crabs\n",
    "\n",
    "data, metadata = crabs(\"~/data\")\n",
    "X_train = data[:100, 3:]\n",
    "y_train = data[:100, 1]\n",
    "\n",
    "print(X_train)\n",
    "N = X_train.shape[0]  # number of data points\n",
    "D = X_train.shape[1]  # number of features\n",
    "\n",
    "print(\"Number of data points: {}\".format(N))\n",
    "print(\"Number of features: {}\".format(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   1.17370207e-02,   1.07571341e-05, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.17370207e-02,   1.00000000e+00,   1.62827581e-01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.07571341e-05,   1.62827581e-01,   1.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.00000000e+00,   7.67101049e-01,   1.13379360e-06],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          7.67101049e-01,   9.99511838e-01,   2.76712344e-05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          1.13379360e-06,   2.76712344e-05,   1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype\n",
    "\n",
    "K = ed.rbf(tf.cast(X_train, tf.float32))\n",
    "# # print(\"edward rbf\")\n",
    "# print(type(K))\n",
    "K.eval()\n",
    "# chol = tf.cholesky(K)\n",
    "# chol.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cholesky decomposition was not successful. The input might not be valid.\n\t [[Node: inference_2/sample_2/Cholesky_22 = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](inference_2/sample_2/mul_117)]]\n\nCaused by op 'inference_2/sample_2/Cholesky_22', defined at:\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-131-0e71642c93d2>\", line 13, in <module>\n    inference.run(n_iter=100)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/inference.py\", line 123, in run\n    self.initialize(*args, **kwargs)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/klqp.py\", line 107, in initialize\n    return super(KLqp, self).initialize(*args, **kwargs)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/variational_inference.py\", line 68, in initialize\n    self.loss, grads_and_vars = self.build_loss_and_gradients(var_list)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/klqp.py\", line 146, in build_loss_and_gradients\n    return build_reparam_loss_and_gradients(self, var_list)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/klqp.py\", line 618, in build_reparam_loss_and_gradients\n    qx_copy = copy(qx, scope=scope)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/util/random_variables.py\", line 228, in copy\n    copy(v, dict_swap, scope, True, copy_q, True)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/util/random_variables.py\", line 244, in copy\n    value, dict_swap, scope, True, copy_q, False)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/util/random_variables.py\", line 86, in _copy_default\n    x = copy(x, *args, **kwargs)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/util/random_variables.py\", line 268, in copy\n    new_op = copy(op, dict_swap, scope, True, copy_q, False)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/util/random_variables.py\", line 314, in copy\n    op_def)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cholesky decomposition was not successful. The input might not be valid.\n\t [[Node: inference_2/sample_2/Cholesky_22 = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](inference_2/sample_2/mul_117)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/pp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pp/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pp/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cholesky decomposition was not successful. The input might not be valid.\n\t [[Node: inference_2/sample_2/Cholesky_22 = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](inference_2/sample_2/mul_117)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-0e71642c93d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKLqp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/inference.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, variables, use_coordinator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m       \u001b[0minfo_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/variational_inference.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, feed_dict)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cholesky decomposition was not successful. The input might not be valid.\n\t [[Node: inference_2/sample_2/Cholesky_22 = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](inference_2/sample_2/mul_117)]]\n\nCaused by op 'inference_2/sample_2/Cholesky_22', defined at:\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-131-0e71642c93d2>\", line 13, in <module>\n    inference.run(n_iter=100)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/inference.py\", line 123, in run\n    self.initialize(*args, **kwargs)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/klqp.py\", line 107, in initialize\n    return super(KLqp, self).initialize(*args, **kwargs)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/variational_inference.py\", line 68, in initialize\n    self.loss, grads_and_vars = self.build_loss_and_gradients(var_list)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/klqp.py\", line 146, in build_loss_and_gradients\n    return build_reparam_loss_and_gradients(self, var_list)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/inferences/klqp.py\", line 618, in build_reparam_loss_and_gradients\n    qx_copy = copy(qx, scope=scope)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/util/random_variables.py\", line 228, in copy\n    copy(v, dict_swap, scope, True, copy_q, True)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/util/random_variables.py\", line 244, in copy\n    value, dict_swap, scope, True, copy_q, False)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/util/random_variables.py\", line 86, in _copy_default\n    x = copy(x, *args, **kwargs)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/util/random_variables.py\", line 268, in copy\n    new_op = copy(op, dict_swap, scope, True, copy_q, False)\n  File \"/Users/colbywise/Desktop/ProbablisticProgramming/ProbabilisticProgrammingProject/src/edward/edward/util/random_variables.py\", line 314, in copy\n    op_def)\n  File \"/Users/colbywise/anaconda2/envs/pp/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cholesky decomposition was not successful. The input might not be valid.\n\t [[Node: inference_2/sample_2/Cholesky_22 = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](inference_2/sample_2/mul_117)]]\n"
     ]
    }
   ],
   "source": [
    "from edward.models import Bernoulli, MultivariateNormalTriL\n",
    "from edward.util import rbf\n",
    "\n",
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "f = MultivariateNormalTriL(loc=tf.zeros(N), scale_tril=tf.cholesky(rbf(X)))\n",
    "y = Bernoulli(logits=f)\n",
    "\n",
    "qf = Normal(loc=tf.Variable(tf.random_normal([N])),\n",
    "            scale=tf.nn.softplus(tf.Variable(tf.random_normal([N]))))\n",
    "\n",
    "inference = ed.KLqp({f: qf}, data={X: X_train, y: y_train})\n",
    "inference.run(n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manhattan = add_all_time(manhattan)\n",
    "manhattan = manhattan.dropna()\n",
    "x = manhattan.drop([\"Unnamed: 0\",\"id\", \"vendor_id\", \n",
    "                             \"store_and_fwd_flag\",\n",
    "                             \"pickup_boro_code\", \n",
    "                             \"dropoff_boro_code\",\n",
    "                             \"dropoff_neighborhood_code\",\n",
    "                             \"pickup_neighborhood_code\",\n",
    "                             \"pickup_datetime\",\n",
    "                             \"pickup_neighborhood_name\",\n",
    "                             \"dropoff_neighborhood_name\",\n",
    "                             \"pickup_boro\",\n",
    "                             \"dropoff_boro\",\n",
    "                             \"dropoff_timestamp\"],\n",
    "                              axis=1)\n",
    "\n",
    "y = manhattan[\"trip_duration\"]\n",
    "manhattan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the timestamp value was too large and causing divergence\n",
    "x.loc[:, \"pickup_timestamp\"] = x[\"pickup_timestamp\"] - x[\"pickup_timestamp\"].mean()\n",
    "x.reset_index(inplace=True, drop=True)\n",
    "# note that y's indices need to be reset differently because it is actually a series rather than dataframe\n",
    "y = y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(x.shape[0], size=int(x.shape[0] * 0.1), replace=False)\n",
    "\n",
    "x_test = x.iloc[train_indices, :]\n",
    "y_test = y.iloc[train_indices]\n",
    "\n",
    "\n",
    "x_train = x.drop(train_indices)\n",
    "y_train = y.drop(train_indices)\n",
    "\n",
    "# convert y to minutes instead of seconds\n",
    "y_train = y_train / 60\n",
    "y_test = y_test / 60\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "# print(\"\\n x_train\\n\", x_train[0:5])\n",
    "# print(\"\\n y_train\\n\", y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = x_train.shape\n",
    "X = tf.placeholder(tf.float32, [None, D])\n",
    "w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "b = Normal(loc=[0.0], scale=1.0)\n",
    "Y = Normal(loc=ed.dot(X, w) + b, scale=1.0)\n",
    "\n",
    "qw = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([D])),\n",
    "                                       scale=tf.Variable(tf.random_normal([D])))\n",
    "\n",
    "qb = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([1])),\n",
    "                                       scale=tf.Variable(tf.random_normal([1])))\n",
    "\n",
    "inference = ed.KLqp({w: qw, b: qb}, data={X: x_train.as_matrix(), Y: y_train.as_matrix()})\n",
    "inference.run(n_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_post = Normal(loc=ed.dot(X, qw) + qb, scale=1.0)\n",
    "# only manhattan distance =>  776342.81\n",
    "# + pickup timestamp      => 7317449.0\n",
    "# - pickup timestamp\n",
    "# + pickup hour           =>  225920.56\n",
    "# + passenger count       =>  222533.69\n",
    "ed.evaluate(\"mean_absolute_error\", data={X: x_test.as_matrix(),\n",
    "                                        y_post: y_test.as_matrix()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph distance to trip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"manhattan_distance\", y=\"trip_duration\", data=ues_to_msh, fit_reg=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets model this with a simple GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ues_to_msh = ues_to_msh.dropna()\n",
    "x = ues_to_msh.drop([\"Unnamed: 0\",\"id\", \"vendor_id\", \"pickup_datetime\",\n",
    "                             \"store_and_fwd_flag\", \"trip_duration\",\n",
    "                             \"pickup_boro\", \"pickup_boro_code\", \"pickup_neighborhood_name\",\n",
    "                             \"pickup_neighborhood_code\", \"dropoff_boro\", \"dropoff_boro_code\",\n",
    "                             \"dropoff_neighborhood_name\", \"dropoff_neighborhood_code\",\n",
    "                             \"dropoff_timestamp\", \"dropoff_datetime\",\n",
    "                             \"dropoff_hour\"], axis=1)\n",
    "y = ues_to_msh[\"trip_duration\"]\n",
    "\n",
    "# the timestamp value was too large and causing divergence\n",
    "x.loc[:, \"pickup_timestamp\"] = x[\"pickup_timestamp\"] - x[\"pickup_timestamp\"].mean()\n",
    "x.reset_index(inplace=True, drop=True)\n",
    "# note that y's indices need to be reset differently because it is actually a series rather than dataframe\n",
    "y = y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(x.shape[0], size=int(x.shape[0] * 0.1), replace=False)\n",
    "\n",
    "x_train = x.iloc[train_indices, :]\n",
    "train_ind_5pm = x_train.where(x[\"pickup_hour\"] == 5).dropna()\n",
    "print(train_ind_5pm)\n",
    "return\n",
    "x_train_5pm_pickup = x[train_ind_5pm]\n",
    "y_train = y.iloc[train_indices]\n",
    "\n",
    "\n",
    "x_test = x.drop(train_indices)\n",
    "y_test = y.drop(train_indices)\n",
    "\n",
    "# convert y to minutes instead of seconds\n",
    "y_train = y_train / 60\n",
    "y_test = y_test / 60\n",
    "\n",
    "x_train = x_train.loc[:, [\"manhattan_distance\", \"pickup_hour\", \"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]]\n",
    "# x_train = x_train.loc[:, [\"manhattan_distance\", \"pickup_hour\", \"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]]\n",
    "x_test = x_test.loc[:, [\"manhattan_distance\", \"pickup_hour\", \"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]]\n",
    "\n",
    "N, D = x_train.shape\n",
    "X = tf.placeholder(tf.float32, [None, D])\n",
    "w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "b = Normal(loc=[0.0], scale=1.0)\n",
    "Y = Normal(loc=ed.dot(X, w) + b, scale=1.0)\n",
    "\n",
    "qw = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([D])),\n",
    "                                       scale=tf.Variable(tf.random_normal([D])))\n",
    "\n",
    "qb = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([1])),\n",
    "                                       scale=tf.Variable(tf.random_normal([1])))\n",
    "\n",
    "inference = ed.KLqp({w: qw, b: qb}, data={X: x_train.as_matrix(), Y: y_train.as_matrix()})\n",
    "inference.run(n_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "  # this is an NxD matrix, where N is number of items and D its dimensionalites\n",
    "sigma = 1.0\n",
    "pairwise_sq_dists = squareform(pdist(x_train, 'sqeuclidean'))\n",
    "print(pairwise_sq_dists)\n",
    "\n",
    "print(np.exp(-pairwise_sq_dists))\n",
    "K = np.exp(-pairwise_sq_dists / sigma**2)\n",
    "L = np.linalg.cholesky(K + 1e-15*np.eye(n))\n",
    "print(L)\n",
    "print(x_train.shape)\n",
    "\n",
    "K = ed.rbf(tf.cast(x_train, tf.float32))\n",
    "K.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from edward.models import Bernoulli, MultivariateNormalTriL\n",
    "from edward.util import rbf\n",
    "\n",
    "\n",
    "N, D = x_train.shape\n",
    "\n",
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "#f = MultivariateNormalTriL(loc=tf.zeros(N), scale_tril=tf.cholesky(rbf(tf.cast(x_train, tf.float32))))\n",
    "f = MultivariateNormalTriL(loc=tf.zeros(N), scale_tril=(tf.cast(L, tf.float32)))\n",
    "Y = Normal(loc=tf.zeros(N), scale=f)\n",
    "\n",
    "qf = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([N])),\n",
    "                                       scale=tf.Variable(tf.random_normal([N])))\n",
    "\n",
    "\n",
    "inference = ed.KLqp({f: qf}, data={X: x_train.as_matrix(), Y: y_train.as_matrix()})\n",
    "inference.run(n_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_post = Normal(loc=ed.dot(X, qw) + qb, scale=1.0)\n",
    "# only manhattan distance =>  776342.81\n",
    "# + pickup timestamp      => 7317449.0\n",
    "# - pickup timestamp\n",
    "# + pickup hour           =>  225920.56\n",
    "# + passenger count       =>  222533.69\n",
    "ed.evaluate(\"mean_absolute_error\", data={X: x_test.as_matrix(),\n",
    "                                        y_post: y_test.as_matrix()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of a Linear Model\n",
    "On average we have an absolute error of about 4:30 for each trip"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pp",
   "language": "python",
   "name": "pp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
